%!TEX root = ../report.tex

\section{Index Construction}
\label{sec:indexing}
When parsing the data set of hundreds of documents we search for the HEADLINE and TEXT tags, which contain content for each document. During this process the Document ID is noted when encountering the DOCID tag, and all other tags are skipped.

\subsection*{Document ID Assignment}

Internal Document IDs are assigned sequentially as they are encountered in the data set using a separate DocIdHandler class, which maintains a dynamically sized array of "raw" Document IDs from the data set (implemented using the standard Java ArrayList collection). The internal Document ID is an integer corresponding to the array index of the "raw" Document ID within the DocIdHandler class, to allow constant time lookups.

\subsection*{Parsing}

Parsing content character by charater is performed in accordance with the following set of rules:

\begin{enumerate}
	\item When reaching mark-up tags such as <p>, skip them
	\item When parsing a token starting with a letter, only allow letters, numbers, and hyphens as non-terminators of the token. When reaching a terminator for a token containing a hyphen, the word is analyzed and handled in the following way:
	\begin{enumerate}
		\item If there is only one hyphen in the word and the first part is a common prefix such as co, pre, meta, or multi, the hyphen is removed and the two parts are concatenated into one\,\cite{ibm13}\cite{grammar13}.
		\item If there is only one hyphen in the word and the last term ends on 'ed' (e.g. case-based) the hyphen is kept\,\cite{ibm13}.
		\item Otherwise all hyphens in the token are removed and a token with $n$ hyphens becomes $n+1$ tokens with no hyphens.
	\end{enumerate}
	\item When parsing a token starting with a digit, only allow letters, digits, dots, and commas as non-terminators of the token. If commas or dots are at the end of a token they are removed.
	\item If the token is less than three characters long, it is thrown away.
\end{enumerate}

Note that this means that the space character will most often be the terminator of a token and that we do not handle acronyms in any special way. As described by Manning et al.\,\cite[p. 24]{manning2008introduction} handling hyphens is often done in accordance with some heuristics that keep or remove hyphens based on various attributes of a token, e.g. its length. Our parser is simplified and will produce unwanted results for sequences such as "San Francisco-Los Angeles". It will, however, manage to index words like "co-operative/cooperative" under the same term ID.

\subsection*{Inverted Index Construction in Memory}

After each document has been parsed, the tokens that were gathered are passed to the indexer module as a term list $(t, f_{d,t})$, where $t$ denotes the term and $f_{d,t}$ denotes the term frequency, in this context the within-document frequency of the term.

The in-memory representation of the inverted index is a HashMap that maps each term $t$ to a document frequency $f_t$ and a list of postings of the form $(d, f_{d,t})$, where $d$ denotes the Document ID and $f_{d,t}$ again denotes the within-document frequency of the term. This, in turn, is represented as an ArrayList, sorted by Document ID.

Since our postings list insertion method inserts new postings in sort order, it guarantees a constant-time insertion of a document where the Document ID is greater than all existing Document IDs, and a linear-time insertion in other cases. We consistently provide a constant-time insertion in our parser implementation, as Document IDs are assigned sequentially and therefore insertion into the postings list is always done at the end.

Initially we tried passing a raw term list for each document to the indexer. Aggregating the terms within the parser provides a dramatic performance improvement when adding the terms to the inverted index, however, as each term requires only a single insertion into the postings list.

\subsection*{Inverted Index Representation on Disk}

The inverted index implementation was inspired by Manning et al.\,\cite{manning2008introduction}.

We produce a lexicon, where each entry consists of the term $t$, the document frequency $f_t$, and the byte offset in our inverted list file. These are separated by the pipe character and each term entry is separated by a newline.

The inverted list is saved on disk as a binary file consisting of non-delimited binary integer values, to be retrieved via the byte offset stored in the lexicon. If a term $t$ occurs in document $d$ there will be a posting of two integers $(d, f_{d,t})$ denoting the Document ID followed by the within-document frequency of $t$.

