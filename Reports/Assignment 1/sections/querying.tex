%!TEX root = ../report.tex

\section{Index Search}
\label{sec:querying}

\subsection*{Parsing of Query Terms}

Our index search implementation first reads in the search terms from the command line and runs them through the same parser used to construct the inverted index. This is to ensure the same tokenisation and normalisation processes are applied to the search terms. Without this step search terms provided may not match the representation of an initially identical indexed term.

\subsection*{Reading in the Lexicon}

The lexicon of the inverted index is read into a standard Java HashMap collection, mapping a term $t$ to term data $(f_t, p_t, s_t)$, where $f_t$ is the document frequency, $p_t$ is the postings list address in the inverted list file, and $s_t$ is the postings list size in bytes as it is stored in the inverted list file. The postings list address is the byte offset of the postings list in the inverted list file.

Implementing the lexicon as a HashMap provides a constant time lookup for each term's frequency data, postings list address and postings list size.

\subsection*{Reading in the Postings List}

The inverted list file is opened for random access using a Java SeekableByteChannel, which allows us to seek to arbitrary points in the file and retrieve multiple bytes at a time\,\cite{seekablebytechannel}.

Once a search term is parsed, the postings list address, postings list byte size and document frequency are retrieved from the lexicon hash.

To retrieve the postings list we first seek to the byte offset stored in the postings list address for the term in the inverted list file. Each posting is made up of two variable byte encoded integers. Due to the variable byte encoding the number of bytes to read from disk cannot be deduced from the document frequency of the term. To improve efficiency the byte size of the postings list is stored in our lexicon, allowing us to avoid multiple disk reads by reading all bytes for the postings list into a buffer for parsing.

The buffer is first decoded to produce a list of integers. This list is parsed to construct a postings list consisting of postings of the form $(d, f_{d,t})$, denoting the Document ID followed by the within-document frequency of $t$. The postings list is passed back to the query program as a search result for further processing.

\subsection*{Retrieving the Raw Document ID}

The raw Document ID mapping is read from the map file into the DocIdHandler class referred to in Section \ref{sec:indexing}. The DocIdHandler stores the raw Document IDs in an array, where the raw Document ID resides in the array index of its corresponding internal Document ID. The DocIdHandler handles this representation internally, and simply converts an internal Document ID to a raw Document ID when requested.

Once the postings list is generated, this list is stepped over. Each raw Document ID is retrieved from the DocIdHandler and printed to the command line with the within-document frequency.

