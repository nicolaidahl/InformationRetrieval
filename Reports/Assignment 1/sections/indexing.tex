%!TEX root = ../report.tex

\section{Indexing}
\label{sec:indexing}
When parsing the data set of houndreds of documents we search for the HEADLINE and TEXT tags, which contain content for each document. During this process the Document ID is noted when encountering the DOCID tag, and all other tags are skipped. Parsing content character by charater is performed in accordance with the following set of rules:

\begin{enumerate}
	\item When reaching mark-up tags such as $<p>$, skip them
	\item When parsing a token starting with a letter, only allow letters, numbers, and hyphens as non-terminators of the token. When reaching a terminator for a token containing a hyphen, the word is analyzed and handled in the following way:
	\begin{enumerate}
		\item If there is only one hyphen in the word and the first part is co, pre, meta, or multi, the hyphen is removed and the two parts are concatenated into one\,\cite{ibm13}.
		\item If there is only one hyphen in the word and the last term ends on 'ed' (e.g. case-based) the hyphen is kept\,\cite{ibm13}.
		\item Otherwise all hyphens in the token are removed and a token with $n$ hyphens becomes $n+1$ tokens with no hyphens.
	\end{enumerate}
	\item When parsing a token starting with a number, only allow letters, numbers, dots, and commas as non-terminators of the token.
	\item If the token is less than three characters long, it is thrown away.
\end{enumerate}

Note that this means that the space character will most often be the terminator of a token and that we do not handle acronyms in any special way. As described by Manning et al.\,\cite[p. 24]{manning2008introduction} handling hyphens is often done in accordance with some heuristics that keep or remove hyphens based on various attributes of a token, e.g. its length. Our parser is simplified and will produce unwanted results for sequences such as "San Francisco-Los Angeles". It will, however, manage to index words like "co-operative/coorperative" under the same term ID.

After each document has been parsed, the tokens that were gathered are passed to the indexer module, that builds our in-memory lexicon and inverted list. The in-memory representation of the inverted index is a HashMap that maps each term to its postings list that, in turn, is represented as an ArrayList. By using these data structures we achieve a constant-time insertion for each token. We guarantee this, as insertion into the postings list is always done to the end of the list. Note that the list will stay sorted, as we assign our own increasing Document IDs.

The inverted index implementation was inspired by Manning et al.\,\cite{manning2008introduction}. We produce a lexicon, where each entry consists of the term, the total term frequency in the data set, and the byte offset into our inverted list where the postings for the term is kept. These are separated by the pipe character and each entry is on its own line. The inverted list is saved on disk as raw, unformatted bytes. If a term $t$ occurs in document $d$ there will be a posting $(d, f_{d,t})$ denoting the Document ID followed by the in-document frequency of $t$.


